{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341dedbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279eb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.6\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()  # Automatically sets SPARK_HOME and initializes PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "sc = spark.sparkContext\n",
    "print(sc.version)  # Should print the Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d805440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual data in dataframe\n",
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of students  data\n",
    "data = [[\"1\", \"sravan\", \"vignan\"], [\"2\", \"ojaswi\", \"vvit\"],\n",
    "        [\"3\", \"rohith\", \"vvit\"], [\"4\", \"sridevi\", \"vignan\"],\n",
    "        [\"1\", \"sravan\", \"vignan\"], [\"5\", \"gnanesh\", \"iit\"]]\n",
    "\n",
    "# specify column names\n",
    "columns = ['student ID', 'student NAME', 'college']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Actual data in dataframe\")\n",
    "# show dataframe\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f05402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|student ID|\n",
      "+----------+\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         1|\n",
      "|         5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column name\n",
    "dataframe.select('student ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7b28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select multiple column with column name\n",
    "dataframe.select(['student ID', 'student NAME', 'college']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970978b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|student NAME|\n",
      "+------------+\n",
      "|      sravan|\n",
      "|      ojaswi|\n",
      "|      rohith|\n",
      "|     sridevi|\n",
      "|      sravan|\n",
      "|     gnanesh|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column number 1\n",
    "dataframe.select(dataframe.columns[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ddb49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt: select column all in  df\n",
    "\n",
    "dataframe.select(*dataframe.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f2d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column number slice\n",
    "# operator\n",
    "dataframe.select(dataframe.columns[0:3]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a397c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+-----------+\n",
      "|col1|       col2|\n",
      "+----+-----------+\n",
      "|   1|        ABC|\n",
      "|   2|        DEF|\n",
      "|   3|        GHI|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using withColumn to remove white spaces\n",
    "df = df.withColumn(\"col2\", rtrim(col(\"col2\")))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ffe0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using select to remove white spaces\n",
    "df = df.select(col(\"col1\"), trim(col(\"col2\")).alias(\"col2\"))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef87a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n",
      "+----+-------+\n",
      "|col1|   col2|\n",
      "+----+-------+\n",
      "|   1|ABC    |\n",
      "|   2|    DEF|\n",
      "|   3|GHI    |\n",
      "+----+-------+\n",
      "\n",
      "+----+-----------+\n",
      "|col1|       col2|\n",
      "+----+-----------+\n",
      "|   1|        ABC|\n",
      "|   2|        DEF|\n",
      "|   3|        GHI|\n",
      "+----+-----------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim\n",
    "\n",
    "# Create a Spark session if one does not exist\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "df.show()\n",
    "\n",
    "\n",
    "# using withColumn and trim()\n",
    "df.withColumn(\"col2\", trim(\"col2\")).show()\n",
    "\n",
    "# using ltrim()\n",
    "df.withColumn(\"col2\", ltrim(\"col2\")).show()\n",
    "\n",
    "# using rtrim()\n",
    "df.withColumn(\"col2\", rtrim(\"col2\")).show()\n",
    "\n",
    "# Using select\n",
    "df.select(\"col1\", trim(\"col2\").alias('col2')).show()\n",
    "\n",
    "# Using SQL Expression\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select col1,trim(col2) as col2 from TAB\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86cb5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+----+\n",
      "|             Country|Country Code|Data|Year|\n",
      "+--------------------+------------+----+----+\n",
      "|               India|          91|2701|2020|\n",
      "|United States of ...|           1|1301|2020|\n",
      "|              Israel|         972|3102|2020|\n",
      "|               Dubai|         971|2901|2020|\n",
      "|              Russia|           7|3101|2020|\n",
      "+--------------------+------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "\n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "    spk = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Substring.com\") \\\n",
    "        .getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_df(spark, data, schema):\n",
    "\n",
    "    df1 = spark.createDataFrame(data, schema)\n",
    "    return df1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"India\", +91, 2701, 2020),\n",
    "                  (\"United States of America\", +1, 1301, 2020),\n",
    "                  (\"Israel\", +972, 3102, 2020),\n",
    "                  (\"Dubai\", +971, 2901, 2020),\n",
    "                  (\"Russia\", 7, 3101, 2020)]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"Country\", \"Country Code\",\n",
    "              \"Data\", \"Year\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fa46cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using withColumn to remove white spaces\n",
    "df = df.withColumn(\"col2\", trim(col(\"col2\")))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc8850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|seq|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"seq\", StringType(), True)])\n",
    "\n",
    "# The data should be a list of rows, where each row is a list or tuple\n",
    "dates = [['1']] # Changed from ['1']\n",
    "\n",
    "df = spark.createDataFrame(dates, schema=schema)\n",
    "\n",
    "df.show()  # Display the DataFrame content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57ecc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n",
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n",
      "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow \n",
    "\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "\n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "\n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n",
    "\n",
    "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
    "print(test)\n",
    "\n",
    "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
    "print(test123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d55402e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark.sql.functions import current_date,year\n",
    " \n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    " \n",
    "from datetime import datetime, date\n",
    " \n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import current_date,year\n",
    "\n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
