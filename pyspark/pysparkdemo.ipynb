{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341dedbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279eb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.6\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()  # Automatically sets SPARK_HOME and initializes PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "sc = spark.sparkContext\n",
    "print(sc.version)  # Should print the Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc8850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|seq|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"seq\", StringType(), True)])\n",
    "\n",
    "# The data should be a list of rows, where each row is a list or tuple\n",
    "dates = [['1']] # Changed from ['1']\n",
    "\n",
    "df = spark.createDataFrame(dates, schema=schema)\n",
    "\n",
    "df.show()  # Display the DataFrame content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 25.8/25.8 MB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n",
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n",
      "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow \n",
    "\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "\n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "\n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n",
    "\n",
    "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
    "print(test)\n",
    "\n",
    "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
    "print(test123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55402e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark.sql.functions import current_date,year\n",
    " \n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    " \n",
    "from datetime import datetime, date\n",
    " \n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import current_date,year\n",
    "\n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
