{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341dedbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279eb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.6\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()  # Automatically sets SPARK_HOME and initializes PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "sc = spark.sparkContext\n",
    "print(sc.version)  # Should print the Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d805440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual data in dataframe\n",
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of students  data\n",
    "data = [[\"1\", \"sravan\", \"vignan\"], [\"2\", \"ojaswi\", \"vvit\"],\n",
    "        [\"3\", \"rohith\", \"vvit\"], [\"4\", \"sridevi\", \"vignan\"],\n",
    "        [\"1\", \"sravan\", \"vignan\"], [\"5\", \"gnanesh\", \"iit\"]]\n",
    "\n",
    "# specify column names\n",
    "columns = ['student ID', 'student NAME', 'college']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Actual data in dataframe\")\n",
    "# show dataframe\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f05402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|student ID|\n",
      "+----------+\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         1|\n",
      "|         5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column name\n",
    "dataframe.select('student ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7b28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select multiple column with column name\n",
    "dataframe.select(['student ID', 'student NAME', 'college']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970978b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|student NAME|\n",
      "+------------+\n",
      "|      sravan|\n",
      "|      ojaswi|\n",
      "|      rohith|\n",
      "|     sridevi|\n",
      "|      sravan|\n",
      "|     gnanesh|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column number 1\n",
    "dataframe.select(dataframe.columns[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ddb49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt: select column all in  df\n",
    "\n",
    "dataframe.select(*dataframe.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f2d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student ID|student NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|      sravan| vignan|\n",
      "|         2|      ojaswi|   vvit|\n",
      "|         3|      rohith|   vvit|\n",
      "|         4|     sridevi| vignan|\n",
      "|         1|      sravan| vignan|\n",
      "|         5|     gnanesh|    iit|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column with column number slice\n",
    "# operator\n",
    "dataframe.select(dataframe.columns[0:3]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a397c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+-----------+\n",
      "|col1|       col2|\n",
      "+----+-----------+\n",
      "|   1|        ABC|\n",
      "|   2|        DEF|\n",
      "|   3|        GHI|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using withColumn to remove white spaces\n",
    "df = df.withColumn(\"col2\", rtrim(col(\"col2\")))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ffe0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using select to remove white spaces\n",
    "df = df.select(col(\"col1\"), trim(col(\"col2\")).alias(\"col2\"))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef87a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n",
      "+----+-------+\n",
      "|col1|   col2|\n",
      "+----+-------+\n",
      "|   1|ABC    |\n",
      "|   2|    DEF|\n",
      "|   3|GHI    |\n",
      "+----+-------+\n",
      "\n",
      "+----+-----------+\n",
      "|col1|       col2|\n",
      "+----+-----------+\n",
      "|   1|        ABC|\n",
      "|   2|        DEF|\n",
      "|   3|        GHI|\n",
      "+----+-----------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim\n",
    "\n",
    "# Create a Spark session if one does not exist\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "df.show()\n",
    "\n",
    "\n",
    "# using withColumn and trim()\n",
    "df.withColumn(\"col2\", trim(\"col2\")).show()\n",
    "\n",
    "# using ltrim()\n",
    "df.withColumn(\"col2\", ltrim(\"col2\")).show()\n",
    "\n",
    "# using rtrim()\n",
    "df.withColumn(\"col2\", rtrim(\"col2\")).show()\n",
    "\n",
    "# Using select\n",
    "df.select(\"col1\", trim(\"col2\").alias('col2')).show()\n",
    "\n",
    "# Using SQL Expression\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select col1,trim(col2) as col2 from TAB\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86cb5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+----+\n",
      "|             Country|Country Code|Data|Year|\n",
      "+--------------------+------------+----+----+\n",
      "|               India|          91|2701|2020|\n",
      "|United States of ...|           1|1301|2020|\n",
      "|              Israel|         972|3102|2020|\n",
      "|               Dubai|         971|2901|2020|\n",
      "|              Russia|           7|3101|2020|\n",
      "+--------------------+------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "\n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "    spk = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Substring.com\") \\\n",
    "        .getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_df(spark, data, schema):\n",
    "\n",
    "    df1 = spark.createDataFrame(data, schema)\n",
    "    return df1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"India\", +91, 2701, 2020),\n",
    "                  (\"United States of America\", +1, 1301, 2020),\n",
    "                  (\"Israel\", +972, 3102, 2020),\n",
    "                  (\"Dubai\", +971, 2901, 2020),\n",
    "                  (\"Russia\", 7, 3101, 2020)]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"Country\", \"Country Code\",\n",
    "              \"Data\", \"Year\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fa46cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| DEF|\n",
      "|   3| GHI|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using withColumn to remove white spaces\n",
    "df = df.withColumn(\"col2\", trim(col(\"col2\")))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc8850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|seq|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"seq\", StringType(), True)])\n",
    "\n",
    "# The data should be a list of rows, where each row is a list or tuple\n",
    "dates = [['1']] # Changed from ['1']\n",
    "\n",
    "df = spark.createDataFrame(dates, schema=schema)\n",
    "\n",
    "df.show()  # Display the DataFrame content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57ecc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajitj\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\ajitj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n",
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n",
      "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow \n",
    "\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "\n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "\n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n",
    "\n",
    "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
    "print(test)\n",
    "\n",
    "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
    "print(test123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d55402e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark.sql.functions import current_date,year\n",
    " \n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    " \n",
    "from datetime import datetime, date\n",
    " \n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2c4b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|loc|\n",
      "+----+---+\n",
      "| ram|chi|\n",
      "|anil|ind|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import current_date,year\n",
    "\n",
    "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark import SparkContext\n",
    " \n",
    "sc=SparkContext.getOrCreate()\n",
    " \n",
    "rdd=sc.parallelize([('ram','chi'),\n",
    " \n",
    "             ('anil','ind')])\n",
    " \n",
    "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
    " \n",
    " \n",
    " \n",
    "#Trying to see the data but face below error:\n",
    " \n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0914fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|col1|           col2|\n",
      "+----+---------------+\n",
      "|   1|        ABC    |\n",
      "|   2|            DEF|\n",
      "|   3|        GHI    |\n",
      "+----+---------------+\n",
      "\n",
      "+----+-----------+\n",
      "|col1|       col2|\n",
      "+----+-----------+\n",
      "|   1|        ABC|\n",
      "|   2|        DEF|\n",
      "|   3|        GHI|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
    "\n",
    "# Define your data\n",
    "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show the initial DataFrame\n",
    "df.show()\n",
    "\n",
    "# Using withColumn to remove white spaces\n",
    "df = df.withColumn(\"col2\", rtrim(col(\"col2\")))\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e31ac4",
   "metadata": {},
   "source": [
    "How to Get substring from a column in PySpark Dataframe ?\n",
    "\n",
    "We can get the substring of the column using substring() and substr() function.\n",
    "\n",
    "\n",
    "Syntax: substring(str,pos,len)\n",
    "\n",
    "\n",
    "df.col_name.substr(start, length)\n",
    "\n",
    "\n",
    "Parameter:\n",
    "\n",
    "\n",
    "str - It can be string or name of the column from which we are getting the substring.\n",
    "start and pos - Through this parameter we can give the starting position from where substring is start.\n",
    "length and len - It is the length of the substring from the starting position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c1c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+----+\n",
      "|             Country|Country Code|Data|Year|\n",
      "+--------------------+------------+----+----+\n",
      "|               India|          91|2701|2020|\n",
      "|United States of ...|           1|1301|2020|\n",
      "|              Israel|         972|3102|2020|\n",
      "|               Dubai|         971|2901|2020|\n",
      "|              Russia|           7|3101|2020|\n",
      "+--------------------+------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "\n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "    spk = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Substring.com\") \\\n",
    "        .getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_df(spark, data, schema):\n",
    "\n",
    "    df1 = spark.createDataFrame(data, schema)\n",
    "    return df1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"India\", +91, 2701, 2020),\n",
    "                  (\"United States of America\", +1, 1301, 2020),\n",
    "                  (\"Israel\", +972, 3102, 2020),\n",
    "                  (\"Dubai\", +971, 2901, 2020),\n",
    "                  (\"Russia\", 7, 3101, 2020)]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"Country\", \"Country Code\",\n",
    "              \"Data\", \"Year\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccff2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Country Code: long (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      "\n",
      "+------------------------+------------+----+-----+----+\n",
      "|Country                 |Country Code|Year|Month|Date|\n",
      "+------------------------+------------+----+-----+----+\n",
      "|India                   |91          |2020|27   |01  |\n",
      "|United States of America|1           |2020|13   |01  |\n",
      "|Israel                  |972         |2020|31   |02  |\n",
      "|Dubai                   |971         |2020|29   |01  |\n",
      "|Russia                  |7           |2020|31   |01  |\n",
      "+------------------------+------------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    # creating Month column and get the\n",
    "    # substring from the Data column\n",
    "    # creating Date column and get the\n",
    "    # substring from the Data column\n",
    "    df = df.withColumn(\n",
    "      \"Month\", substring(\"Data\", 1, 2)).withColumn(\n",
    "      \"Date\", substring(\"Data\", 3, 4))\n",
    "\n",
    "    # dropping the Data column from the\n",
    "    # Dataframe\n",
    "    df = df.drop(\"Data\")\n",
    "\n",
    "    # printing Dataframe schema to get the\n",
    "    # column names\n",
    "    df.printSchema()\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1d83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Country Code: long (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- New_Country: string (nullable = true)\n",
      "\n",
      "+------------------------+------------+----+-----+----+------------+\n",
      "|Country                 |Country Code|Year|Month|Date|New_Country |\n",
      "+------------------------+------------+----+-----+----+------------+\n",
      "|India                   |91          |2020|27   |01  |India       |\n",
      "|United States of America|1           |2020|13   |01  |United State|\n",
      "|Israel                  |972         |2020|31   |02  |Israel      |\n",
      "|Dubai                   |971         |2020|29   |01  |Dubai       |\n",
      "|Russia                  |7           |2020|31   |01  |Russia      |\n",
      "+------------------------+------------+----+-----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Creating the new column New_Country\n",
    "    # and store the substring using substr()\n",
    "    df = df.withColumn(\"New_Country\", df.Country.substr(0, 12))\n",
    "\n",
    "    # printing Dataframe schema to get the\n",
    "    # column names\n",
    "    df.printSchema()\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfa3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+\n",
      "|Name       |First Name|Last Name|\n",
      "+-----------+----------+---------+\n",
      "|AidanButler|Aidan     |Butler   |\n",
      "|ConerFlores|Coner     |Flores   |\n",
      "|RosseBryant|Rosse     |Bryant   |\n",
      "|JuliaSimmon|Julia     |Simmon   |\n",
      "|AliceBailey|Alice     |Bailey   |\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"India\", +91, \"AidanButler\"),\n",
    "                  (\"United States of America\", +1, \"ConerFlores\"),\n",
    "                  (\"Israel\", +972, \"RosseBryant\"),\n",
    "                  (\"Dubai\", +971, \"JuliaSimmon\"),\n",
    "                  (\"Russia\", 7, \"AliceBailey\")]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"Country\", \"Country Code\", \"Name\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "\n",
    "    # Selecting the column using select()\n",
    "    # function and getting substring\n",
    "    # using substring()\n",
    "    df2 = df.select('Name', substring('Name',\n",
    "                                      1, 5).alias('First Name'),\n",
    "                    substring('Name', 6, 6).alias('Last Name'))\n",
    "\n",
    "    # printing Dataframe schema to get the column names\n",
    "    df2.printSchema()\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9e5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+\n",
      "|Name       |First_Name|Last_Name|\n",
      "+-----------+----------+---------+\n",
      "|AidanButler|Aidan     |Butler   |\n",
      "|ConerFlores|Coner     |Flores   |\n",
      "|RosseBryant|Rosse     |Bryant   |\n",
      "|JuliaSimmon|Julia     |Simmon   |\n",
      "|AliceBailey|Alice     |Bailey   |\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"India\", +91, \"AidanButler\"),\n",
    "                  (\"United States of America\", +1, \"ConerFlores\"),\n",
    "                  (\"Israel\", +972, \"RosseBryant\"),\n",
    "                  (\"Dubai\", +971, \"JuliaSimmon\"),\n",
    "                  (\"Russia\", 7, \"AliceBailey\")]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"Country\", \"Country Code\", \"Name\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "\n",
    "    # Selecting the column using selectExpr()\n",
    "    # function and getting substring using substring()\n",
    "    df2 = df.selectExpr('Name', 'substring(Name, 1,5) as First_Name',\n",
    "                        'substring(Name, 6,6) as Last_Name')\n",
    "\n",
    "    # printing Dataframe schema to get the column names\n",
    "    df2.printSchema()\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdbc66",
   "metadata": {},
   "source": [
    "Pivot String column on Pyspark Dataframe\n",
    "\n",
    "Pivoting in data analysis refers to the transformation of data from a long format to a wide format by rotating rows into columns. In PySpark, pivoting is used to restructure DataFrames by turning unique values from a specific column (often categorical) into new columns, with the option to aggregate values based on another column. This transformation is particularly useful for summarizing and analyzing data, allowing us to easily compare and analyze distinct attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7db1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|      date| product|sales|\n",
      "+----------+--------+-----+\n",
      "|2024-01-01|ProductA|   10|\n",
      "|2024-01-01|ProductB|   20|\n",
      "|2024-01-02|ProductA|   30|\n",
      "|2024-01-02|ProductB|   40|\n",
      "+----------+--------+-----+\n",
      "\n",
      "Dataframe after Pivot Operation\n",
      "+----------+--------+--------+\n",
      "|      date|ProductA|ProductB|\n",
      "+----------+--------+--------+\n",
      "|2024-01-02|      30|      40|\n",
      "|2024-01-01|      10|      20|\n",
      "+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ('2024-01-01', 'ProductA', 10),\n",
    "    ('2024-01-01', 'ProductB', 20),\n",
    "    ('2024-01-02', 'ProductA', 30),\n",
    "    ('2024-01-02', 'ProductB', 40),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, ['date', 'product', 'sales'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Pivot DataFrame\n",
    "pivot_df = df.groupBy('date').pivot('product').agg(sum('sales'))\n",
    "\n",
    "print(\"Dataframe after Pivot Operation\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c74dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "    spk = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Product_details.com\") \\\n",
    "        .getOrCreate()\n",
    "    return spk\n",
    "\n",
    "def create_df(spark, data, schema):\n",
    "    df1 = spark.createDataFrame(data, schema)\n",
    "    return df1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_data = [(\"Uttar Pradesh\", 122000, 89600, 12238),\n",
    "                  (\"Maharashtra\", 454000, 380000, 67985),\n",
    "                  (\"Tamil Nadu\", 115000, 102000, 13933),\n",
    "                  (\"Karnataka\", 147000, 111000, 15306),\n",
    "                  (\"Kerala\", 153000, 124000, 5259)]\n",
    "\n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    "\n",
    "    schema = [\"State\", \"Cases\", \"Recovered\", \"Deaths\"]\n",
    "\n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652cf515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of column names: ['State', 'Cases', 'Recovered', 'Deaths']\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the list of column names \n",
    "col = df.columns\n",
    "\n",
    "# printing\n",
    "print(f'List of column names: {col}')\n",
    "\n",
    "# visualizing the dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3518f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - State\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n",
      "2 - Cases\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n",
      "3 - Recovered\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n",
      "4 - Deaths\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the list of StructFields\n",
    "field = df.schema.fields\n",
    "\n",
    "# using for loop to iterate and enumerate\n",
    "# for indexing or numbering\n",
    "for count, col_name in enumerate(field, 1):\n",
    "  \n",
    "    # printing the column names\n",
    "    print(count, \"-\", col_name.name)\n",
    "\n",
    "    # visualizing the dataframe\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99c5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Cases: long (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- Deaths: long (nullable = true)\n",
      "\n",
      "+-------------+------+---------+------+\n",
      "|        State| Cases|Recovered|Deaths|\n",
      "+-------------+------+---------+------+\n",
      "|Uttar Pradesh|122000|    89600| 12238|\n",
      "|  Maharashtra|454000|   380000| 67985|\n",
      "|   Tamil Nadu|115000|   102000| 13933|\n",
      "|    Karnataka|147000|   111000| 15306|\n",
      "|       Kerala|153000|   124000|  5259|\n",
      "+-------------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing Dataframe schema to\n",
    "# get the column names\n",
    "df.printSchema()\n",
    "\n",
    "# visualizing the dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62e2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+------+\n",
      "| ID|       NAME|  sector|salary|\n",
      "+---+-----------+--------+------+\n",
      "|  1|     sravan|      IT| 45000|\n",
      "|  2|     ojaswi|      IT| 30000|\n",
      "|  3|      bobby|business| 45000|\n",
      "|  4|     rohith|      IT| 45000|\n",
      "|  5|    gnanesh|business|120000|\n",
      "|  6|siva nagulu|   sales| 23000|\n",
      "|  7|      bhanu|   sales| 34000|\n",
      "|  8|   sireesha|business|456798|\n",
      "|  9|       ravi|      IT|230000|\n",
      "| 10|       devi|business|100000|\n",
      "+---+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import pyspark\n",
    "\n",
    "# importing sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession and giving an app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# list  of employee data with 10 row values\n",
    "data =[[\"1\",\"sravan\",\"IT\",45000],\n",
    "       [\"2\",\"ojaswi\",\"IT\",30000],\n",
    "       [\"3\",\"bobby\",\"business\",45000],\n",
    "       [\"4\",\"rohith\",\"IT\",45000],\n",
    "       [\"5\",\"gnanesh\",\"business\",120000],\n",
    "       [\"6\",\"siva nagulu\",\"sales\",23000],\n",
    "       [\"7\",\"bhanu\",\"sales\",34000],\n",
    "       [\"8\",\"sireesha\",\"business\",456798],\n",
    "       [\"9\",\"ravi\",\"IT\",230000],\n",
    "       [\"10\",\"devi\",\"business\",100000],\n",
    "       ]\n",
    "\n",
    "# specify column names\n",
    "columns=['ID','NAME','sector','salary']\n",
    "\n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data,columns)\n",
    "\n",
    "# display dataframe\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee38167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|  sector|Employee_salary|\n",
      "+--------+---------------+\n",
      "|   sales|          57000|\n",
      "|      IT|         350000|\n",
      "|business|         721798|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing sum function\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# group the salary among different sectors\n",
    "# and name  as Employee_salary by sum aggregation\n",
    "dataframe.groupBy(\n",
    "  \"sector\").agg(sum(\"salary\").alias(\"Employee_salary\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
