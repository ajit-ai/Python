{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2214de9",
   "metadata": {},
   "source": [
    "Scikit-learn is an open-source machine learning library that provides simple and efficient tools for data analysis and modeling. It is built on NumPy, SciPy, and Matplotlib, making it a powerful tool for tasks like classification, regression, clustering, and dimensionality reduction.\n",
    "\n",
    "Classification: Classification involves teaching a computer to categorize things. For example, a model could be built to determine whether an email is spam or not.\n",
    "Regression: Regression predicting numbers based on other numbers. For instance, a model could predict house prices using factors like location, size, and age.\n",
    "Clustering: Clustering involves finding patterns in data and grouping similar items together. For example, customers could be segmented into different groups based on their shopping habits.\n",
    "Dimensionality Reduction: Dimensionality reduction helps focus on essential data parts while discarding noise. This is useful when dealing with a lot of data that isn't all relevant.\n",
    "Features of Scikit-Learn\n",
    "Scikit-learn is indeed a versatile tool for machine learning tasks, offering a wide range of features to address various aspects of the data science pipeline. let's examine prime key features of scikit-learn:\n",
    "\n",
    "Supervised Learning\n",
    "Classification: Algorithms for predicting categorical labels, including logistic regression, decision trees, random forests, support vector machines (SVMs) and gradient boosting.\n",
    "Regression: Algorithms for predicting continuous outputs, including linear regression, support vector regression, and decision tree regression.\n",
    "Unsupervised Learning\n",
    "Clustering: Techniques for grouping data points into similar clusters, including K-means clustering, DBSCAN, and hierarchical clustering.\n",
    "Dimensionality Reduction: Methods for reducing the number of features in your data, such as principal component analysis (PCA).\n",
    "Data Preprocessing\n",
    "Data Splitting: Functions to split your data into training and testing sets for model evaluation.\n",
    "Feature Scaling: Techniques for normalizing the scale of your features.\n",
    "Feature Selection: Methods to identify and select the most relevant features for your model.\n",
    "Feature Extraction: Tools to create new features from existing ones, such as text vectorization for natural language processing tasks.\n",
    "Model Evaluation\n",
    "Metrics: Functions to calculate performance metrics like accuracy, precision, recall, and F1-score for classification models, and mean squared error (MSE) for regression models.\n",
    "Model Selection: Tools for selecting the best model hyperparameters through techniques like grid search and randomized search.\n",
    "Additional Features\n",
    "Inbuilt datasets: Scikit-learn provides a variety of sample datasets for experimentation and learning purposes.\n",
    "Easy to Use API: Scikit-learn is known for its consistent and user-friendly API, making it accessible to both beginners and experienced data scientists.\n",
    "Open Source: Scikit-learn is an open-source library with a large and active community, ensuring continuous development and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba8d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "###Classification - Logistic Regression Algorithm Example\n",
    "##Logistic Regression is a binary classification algorithm that estimates probabilities of a binary outcome. \n",
    "# It's used for problems like spam detection, medical diagnosis, and credit scoring. \n",
    "# It's chosen for its simplicity, interpretability, and effectiveness in linearly separable datasets.\n",
    "\n",
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de983f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "###Classification - KNN Classifier Algorithm Example\n",
    "##K-Nearest Neighbors (KNN) algorithm classifies data points based on the majority class of their nearest neighbors. \n",
    "# It's useful for simple classification tasks, particularly when data is not linearly separable or when decision boundaries are complex. It's used in recommendation systems, handwriting recognition, and medical diagnosis.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "152bc9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.555891598695242\n"
     ]
    }
   ],
   "source": [
    "###Regression - Linear Regression Algorithm Example\n",
    "##Linear Regression fits a linear model to observed data points, predicting continuous outcomes based on input features. \n",
    "# It's used when exploring relationships between variables and making predictions. Applications include economics, finance, engineering, and social sciences.\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3686d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajitj\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "###Clustering - KMeans Algorithm Example\n",
    "##KMeans algorithm partitions data into k clusters based on similarity. \n",
    "# It's used for unsupervised clustering tasks like customer segmentation, image compression, and anomaly detection. \n",
    "# Ideal when data's structure is unknown but grouping is desired.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Initialize the KMeans clustering model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(iris.data)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "print(\"Cluster Labels:\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (1797, 64)\n",
      "Reduced data shape: (1797, 2)\n"
     ]
    }
   ],
   "source": [
    "###Dimensionality Reduction - PCA Example\n",
    "##PCA (Principal Component Analysis) reduces the dimensionality of data by finding the most important features. \n",
    "# It's used for visualizing high-dimensional data, noise reduction, and speeding up machine learning algorithms. \n",
    "# Commonly applied in image processing, genetics, and finance.\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Initialize PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the data\n",
    "reduced_data = pca.fit_transform(digits.data)\n",
    "\n",
    "print(\"Original data shape:\", digits.data.shape)\n",
    "print(\"Reduced data shape:\", reduced_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
