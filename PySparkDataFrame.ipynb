{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzPvZS6S2K8i2CJ4XL1WKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajit-ai/Python/blob/main/PySparkDataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify the SparkContext\n",
        "sc = spark.sparkContext\n",
        "print(sc.version)  # Should print the Spark version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsRf8NCX69r3",
        "outputId": "796a09b5-e762-4670-e7ab-d472b8afbdf3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "\n",
        "# importing sparksession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving an app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# list  of students  data\n",
        "data = [[\"1\", \"sravan\", \"vignan\"], [\"2\", \"ojaswi\", \"vvit\"],\n",
        "        [\"3\", \"rohith\", \"vvit\"], [\"4\", \"sridevi\", \"vignan\"],\n",
        "        [\"1\", \"sravan\", \"vignan\"], [\"5\", \"gnanesh\", \"iit\"]]\n",
        "\n",
        "# specify column names\n",
        "columns = ['student ID', 'student NAME', 'college']\n",
        "\n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Actual data in dataframe\")\n",
        "# show dataframe\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ-ZrPte2b1D",
        "outputId": "4c41526c-38a4-4211-bdc5-ed4fc5019737"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data in dataframe\n",
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column name\n",
        "dataframe.select('student ID').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt_FZQiK58GK",
        "outputId": "342983a3-7b1e-4a42-e4f5-1683dc8848aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|student ID|\n",
            "+----------+\n",
            "|         1|\n",
            "|         2|\n",
            "|         3|\n",
            "|         4|\n",
            "|         1|\n",
            "|         5|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select multiple column with column name\n",
        "dataframe.select(['student ID', 'student NAME', 'college']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv1WWzj86H45",
        "outputId": "0d5aa5b6-094c-4952-be13-14f67d079c13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column number 1\n",
        "dataframe.select(dataframe.columns[1]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX9pzcQv6wmp",
        "outputId": "d4db6ca6-2768-4898-ad68-cadf30486f30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|student NAME|\n",
            "+------------+\n",
            "|      sravan|\n",
            "|      ojaswi|\n",
            "|      rohith|\n",
            "|     sridevi|\n",
            "|      sravan|\n",
            "|     gnanesh|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: select column all in  df\n",
        "\n",
        "dataframe.select(*dataframe.columns).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmE4cV3S7HBp",
        "outputId": "eebb88c8-2849-4d78-a5e4-cba10721154f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column number slice\n",
        "# operator\n",
        "dataframe.select(dataframe.columns[0:3]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08eA9zJX7kd3",
        "outputId": "1ebce7cf-367f-4d4b-de5c-fa776416c7b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
        "\n",
        "# Define your data\n",
        "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
        "\n",
        "# Show the initial DataFrame\n",
        "df.show()\n",
        "\n",
        "# Using withColumn to remove white spaces\n",
        "df = df.withColumn(\"col2\", rtrim(col(\"col2\")))\n",
        "df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN_oRcan8O1x",
        "outputId": "4aa69b8a-9733-4f08-f9e3-887d0d867f3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|col1|           col2|\n",
            "+----+---------------+\n",
            "|   1|        ABC    |\n",
            "|   2|            DEF|\n",
            "|   3|        GHI    |\n",
            "+----+---------------+\n",
            "\n",
            "+----+-----------+\n",
            "|col1|       col2|\n",
            "+----+-----------+\n",
            "|   1|        ABC|\n",
            "|   2|        DEF|\n",
            "|   3|        GHI|\n",
            "+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
        "\n",
        "# Define your data\n",
        "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
        "\n",
        "# Show the initial DataFrame\n",
        "df.show()\n",
        "\n",
        "# Using withColumn to remove white spaces\n",
        "df = df.withColumn(\"col2\", trim(col(\"col2\")))\n",
        "df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq9P1GaW8Xhq",
        "outputId": "1e28ba0f-6d0f-4583-df09-72bb1a0996d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|col1|           col2|\n",
            "+----+---------------+\n",
            "|   1|        ABC    |\n",
            "|   2|            DEF|\n",
            "|   3|        GHI    |\n",
            "+----+---------------+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
        "\n",
        "# Define your data\n",
        "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
        "\n",
        "# Show the initial DataFrame\n",
        "df.show()\n",
        "\n",
        "# Using select to remove white spaces\n",
        "df = df.select(col(\"col1\"), trim(col(\"col2\")).alias(\"col2\"))\n",
        "df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gPAhyhS9lcC",
        "outputId": "330232ba-c525-437b-c191-fac5808e2e71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|col1|           col2|\n",
            "+----+---------------+\n",
            "|   1|        ABC    |\n",
            "|   2|            DEF|\n",
            "|   3|        GHI    |\n",
            "+----+---------------+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, ltrim, rtrim, col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
        "\n",
        "# Define your data\n",
        "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
        "\n",
        "# Show the initial DataFrame\n",
        "df.show()\n",
        "\n",
        "# Using SQL Expression to remove white spaces\n",
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"SELECT col1, TRIM(col2) AS col2 FROM TAB\").show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8J9NkZI9qIh",
        "outputId": "7f3aebe6-bcd1-45a5-b930-a6c86e209ac7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|col1|           col2|\n",
            "+----+---------------+\n",
            "|   1|        ABC    |\n",
            "|   2|            DEF|\n",
            "|   3|        GHI    |\n",
            "+----+---------------+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, ltrim, rtrim\n",
        "\n",
        "# Create a Spark session if one does not exist\n",
        "spark = SparkSession.builder.appName(\"WhiteSpaceRemoval\").getOrCreate()\n",
        "\n",
        "data = [(1, \"ABC    \"), (2, \"     DEF\"), (3, \"        GHI    \")]\n",
        "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "# using withColumn and trim()\n",
        "df.withColumn(\"col2\", trim(\"col2\")).show()\n",
        "\n",
        "# using ltrim()\n",
        "df.withColumn(\"col2\", ltrim(\"col2\")).show()\n",
        "\n",
        "# using rtrim()\n",
        "df.withColumn(\"col2\", rtrim(\"col2\")).show()\n",
        "\n",
        "# Using select\n",
        "df.select(\"col1\", trim(\"col2\").alias('col2')).show()\n",
        "\n",
        "# Using SQL Expression\n",
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"select col1,trim(col2) as col2 from TAB\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6GF7EOK9zoh",
        "outputId": "6efe83f9-4199-4728-fb23-6677b0dd89f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|col1|           col2|\n",
            "+----+---------------+\n",
            "|   1|        ABC    |\n",
            "|   2|            DEF|\n",
            "|   3|        GHI    |\n",
            "+----+---------------+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n",
            "+----+-------+\n",
            "|col1|   col2|\n",
            "+----+-------+\n",
            "|   1|ABC    |\n",
            "|   2|    DEF|\n",
            "|   3|GHI    |\n",
            "+----+-------+\n",
            "\n",
            "+----+-----------+\n",
            "|col1|       col2|\n",
            "+----+-----------+\n",
            "|   1|        ABC|\n",
            "|   2|        DEF|\n",
            "|   3|        GHI|\n",
            "+----+-----------+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n",
            "+----+----+\n",
            "|col1|col2|\n",
            "+----+----+\n",
            "|   1| ABC|\n",
            "|   2| DEF|\n",
            "|   3| GHI|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, substring\n",
        "\n",
        "\n",
        "# function to create new SparkSession\n",
        "def create_session():\n",
        "    spk = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Substring.com\") \\\n",
        "        .getOrCreate()\n",
        "    return spk\n",
        "\n",
        "def create_df(spark, data, schema):\n",
        "\n",
        "    df1 = spark.createDataFrame(data, schema)\n",
        "    return df1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_data = [(\"India\", +91, 2701, 2020),\n",
        "                  (\"United States of America\", +1, 1301, 2020),\n",
        "                  (\"Israel\", +972, 3102, 2020),\n",
        "                  (\"Dubai\", +971, 2901, 2020),\n",
        "                  (\"Russia\", 7, 3101, 2020)]\n",
        "\n",
        "    # calling function to create SparkSession\n",
        "    spark = create_session()\n",
        "\n",
        "    schema = [\"Country\", \"Country Code\",\n",
        "              \"Data\", \"Year\"]\n",
        "\n",
        "    # calling function to create dataframe\n",
        "    df = create_df(spark, input_data, schema)\n",
        "    df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayCPuCjP-LUo",
        "outputId": "c61e68fa-3e60-4322-a9e6-5c89196e4792"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+----+----+\n",
            "|             Country|Country Code|Data|Year|\n",
            "+--------------------+------------+----+----+\n",
            "|               India|          91|2701|2020|\n",
            "|United States of ...|           1|1301|2020|\n",
            "|              Israel|         972|3102|2020|\n",
            "|               Dubai|         971|2901|2020|\n",
            "|              Russia|           7|3101|2020|\n",
            "+--------------------+------------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_data = [(\"India\", +91, 2701, 2020),\n",
        "                  (\"United States of America\", +1, 1301, 2020),\n",
        "                  (\"Israel\", +972, 3102, 2020),\n",
        "                  (\"Dubai\", +971, 2901, 2020),\n",
        "                  (\"Russia\", 7, 3101, 2020)]\n",
        "\n",
        "    # calling function to create SparkSession\n",
        "    spark = create_session()\n",
        "\n",
        "    schema = [\"Country\", \"Country Code\",\n",
        "              \"Data\", \"Year\"]\n",
        "\n",
        "    # calling function to create dataframe\n",
        "    df = create_df(spark, input_data, schema)\n",
        "\n",
        "    # creating Month column and get the\n",
        "    # substring from the Data column\n",
        "    # creating Date column and get the\n",
        "    # substring from the Data column\n",
        "    df = df.withColumn(\n",
        "      \"Month\", substring(\"Data\", 1, 2)).withColumn(\n",
        "      \"Date\", substring(\"Data\", 3, 4))\n",
        "\n",
        "    # dropping the Data column from the\n",
        "    # Dataframe\n",
        "    df = df.drop(\"Data\")\n",
        "\n",
        "    # printing Dataframe schema to get the\n",
        "    # column names\n",
        "    df.printSchema()\n",
        "\n",
        "    # visualizing the dataframe\n",
        "    df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YRz74z68ASY",
        "outputId": "42572888-1f5f-4109-8935-31487251d286"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Country Code: long (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            "\n",
            "+------------------------+------------+----+-----+----+\n",
            "|Country                 |Country Code|Year|Month|Date|\n",
            "+------------------------+------------+----+-----+----+\n",
            "|India                   |91          |2020|27   |01  |\n",
            "|United States of America|1           |2020|13   |01  |\n",
            "|Israel                  |972         |2020|31   |02  |\n",
            "|Dubai                   |971         |2020|29   |01  |\n",
            "|Russia                  |7           |2020|31   |01  |\n",
            "+------------------------+------------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Creating the new column New_Country\n",
        "    # and store the substring using substr()\n",
        "    df = df.withColumn(\"New_Country\", df.Country.substr(0, 12))\n",
        "\n",
        "    # printing Dataframe schema to get the\n",
        "    # column names\n",
        "    df.printSchema()\n",
        "\n",
        "    # visualizing the dataframe\n",
        "    df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXfTpUR48R9l",
        "outputId": "914edb7d-946e-4ba5-c463-1ff7daa1cbb5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Country Code: long (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- New_Country: string (nullable = true)\n",
            "\n",
            "+------------------------+------------+----+-----+----+------------+\n",
            "|Country                 |Country Code|Year|Month|Date|New_Country |\n",
            "+------------------------+------------+----+-----+----+------------+\n",
            "|India                   |91          |2020|27   |01  |India       |\n",
            "|United States of America|1           |2020|13   |01  |United State|\n",
            "|Israel                  |972         |2020|31   |02  |Israel      |\n",
            "|Dubai                   |971         |2020|29   |01  |Dubai       |\n",
            "|Russia                  |7           |2020|31   |01  |Russia      |\n",
            "+------------------------+------------+----+-----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_data = [(\"India\", +91, \"AidanButler\"),\n",
        "                  (\"United States of America\", +1, \"ConerFlores\"),\n",
        "                  (\"Israel\", +972, \"RosseBryant\"),\n",
        "                  (\"Dubai\", +971, \"JuliaSimmon\"),\n",
        "                  (\"Russia\", 7, \"AliceBailey\")]\n",
        "\n",
        "    # calling function to create SparkSession\n",
        "    spark = create_session()\n",
        "\n",
        "    schema = [\"Country\", \"Country Code\", \"Name\"]\n",
        "\n",
        "    # calling function to create dataframe\n",
        "    df = create_df(spark, input_data, schema)\n",
        "\n",
        "    # Selecting the column using selectExpr()\n",
        "    # function and getting substring using substring()\n",
        "    df2 = df.selectExpr('Name', 'substring(Name, 1,5) as First_Name',\n",
        "                        'substring(Name, 6,6) as Last_Name')\n",
        "\n",
        "    # printing Dataframe schema to get the column names\n",
        "    df2.printSchema()\n",
        "\n",
        "    # visualizing the dataframe\n",
        "    df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eazvBsOP8h-0",
        "outputId": "176a07e9-5625-41d4-88a7-08ce68c4a5a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- First_Name: string (nullable = true)\n",
            " |-- Last_Name: string (nullable = true)\n",
            "\n",
            "+-----------+----------+---------+\n",
            "|Name       |First_Name|Last_Name|\n",
            "+-----------+----------+---------+\n",
            "|AidanButler|Aidan     |Butler   |\n",
            "|ConerFlores|Coner     |Flores   |\n",
            "|RosseBryant|Rosse     |Bryant   |\n",
            "|JuliaSimmon|Julia     |Simmon   |\n",
            "|AliceBailey|Alice     |Bailey   |\n",
            "+-----------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "\n",
        "# importing sparksession from\n",
        "# pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving\n",
        "# app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# giving rows value for dataframe\n",
        "data = [(\"Ram\", \"MCA\", 80),\n",
        "        (\"Riya\", \"MBA\", 85),\n",
        "        (\"Jiya\", \"B.E\", 60),\n",
        "        (\"Maria\", \"B.Tech\", 65),\n",
        "        (\"Shreya\", \"B.sc\", 91),\n",
        "        (\"Ram\", \"MCA\", 80),\n",
        "        (\"John\", \"M.E\", 85),\n",
        "        (\"Shyam\", \"BA\", 70),\n",
        "        (\"Kumar\", \"B.sc\", 78),\n",
        "        (\"Maria\", \"B.Tech\", 65)]\n",
        "\n",
        "# giving column names of dataframe\n",
        "columns = [\"Name\", \"Course\", \"Marks\"]\n",
        "\n",
        "# creating a dataframe df\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# show df\n",
        "df.show()\n",
        "\n",
        "# counting the total number of values\n",
        "# in df\n",
        "print(\"Total number of records in df:\", df.count())"
      ],
      "metadata": {
        "id": "OFDh_rFZFFtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2532d205-247a-4612-88fd-6d6566166e2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+\n",
            "|  Name|Course|Marks|\n",
            "+------+------+-----+\n",
            "|   Ram|   MCA|   80|\n",
            "|  Riya|   MBA|   85|\n",
            "|  Jiya|   B.E|   60|\n",
            "| Maria|B.Tech|   65|\n",
            "|Shreya|  B.sc|   91|\n",
            "|   Ram|   MCA|   80|\n",
            "|  John|   M.E|   85|\n",
            "| Shyam|    BA|   70|\n",
            "| Kumar|  B.sc|   78|\n",
            "| Maria|B.Tech|   65|\n",
            "+------+------+-----+\n",
            "\n",
            "Total number of records in df: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying distinct().count() on df\n",
        "print('Distinct count in DataFrame df is :', df.distinct().count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7yEzXc5wyYz",
        "outputId": "f742a946-aff5-40f8-b66b-7f9c7a8be0a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct count in DataFrame df is : 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing sparksession from\n",
        "# pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving\n",
        "# app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# giving rows value for dataframe\n",
        "data = [(\"Ram\", \"IT\", 80000),\n",
        "        (\"Shyam\", \"Sales\", 70000),\n",
        "        (\"Jiya\", \"Sales\", 60000),\n",
        "        (\"Maria\", \"Accounts\", 65000),\n",
        "        (\"Ramesh\", \"IT\", 80000),\n",
        "        (\"John\", \"Management\", 80000),\n",
        "        (\"Shyam\", \"Sales\", 70000),\n",
        "        (\"Kumar\", \"Sales\", 78000),\n",
        "        (\"Maria\", \"Accounts\", 65000)]\n",
        "\n",
        "# giving column names of dataframe\n",
        "columns = [\"Emp_name\", \"Depart\", \"Salary\"]\n",
        "\n",
        "# creating a dataframe df\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# show df\n",
        "df.show()\n",
        "\n",
        "# counting the total number of values in df\n",
        "print(\"Total number of records in df:\", df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAycYIoDyO0j",
        "outputId": "308f5369-dee2-4a78-934a-50384c7cd675"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|Emp_name|    Depart|Salary|\n",
            "+--------+----------+------+\n",
            "|     Ram|        IT| 80000|\n",
            "|   Shyam|     Sales| 70000|\n",
            "|    Jiya|     Sales| 60000|\n",
            "|   Maria|  Accounts| 65000|\n",
            "|  Ramesh|        IT| 80000|\n",
            "|    John|Management| 80000|\n",
            "|   Shyam|     Sales| 70000|\n",
            "|   Kumar|     Sales| 78000|\n",
            "|   Maria|  Accounts| 65000|\n",
            "+--------+----------+------+\n",
            "\n",
            "Total number of records in df: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing countDistinct from\n",
        "# pyspark.sql.functions\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# applying the function countDistinct()\n",
        "# on df using select()\n",
        "df2 = df.select(countDistinct(\"Emp_name\", \"Depart\", \"Salary\"))\n",
        "\n",
        "# show df2\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpbG7Xbi23Wc",
        "outputId": "5b9eda34-f80b-4514-b418-6f47476fed5e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+\n",
            "|count(DISTINCT Emp_name, Depart, Salary)|\n",
            "+----------------------------------------+\n",
            "|                                       7|\n",
            "+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing countDistinct from\n",
        "# pyspark.sql.functions\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# applying the function countDistinct()\n",
        "# on df using select()\n",
        "df3 = df.select(countDistinct(\"Depart\"))\n",
        "\n",
        "# show df2\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5QZWEGx6enF",
        "outputId": "0a34146e-d857-45a9-cc4b-ff839af1118e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+\n",
            "|count(DISTINCT Depart)|\n",
            "+----------------------+\n",
            "|                     4|\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing sparksession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# giving rows value for dataframe\n",
        "data = [(\"Ram\", \"IT\", 44, 80000),\n",
        "        (\"Shyam\", \"Sales\", 45, 70000),\n",
        "        (\"Jiya\", \"Sales\", 30, 60000),\n",
        "        (\"Maria\", \"Accounts\", 29, 65000),\n",
        "        (\"Ram\", \"IT\", 38, 80000),\n",
        "        (\"John\", \"Management\", 35, 80000),\n",
        "        (\"Shyam\", \"Sales\", 45, 70000),\n",
        "        (\"Kumar\", \"Sales\", 27, 70000),\n",
        "        (\"Maria\", \"Accounts\", 32, 65000),\n",
        "        (\"Ria\", \"Management\", 32, 65000)]\n",
        "\n",
        "# giving column names of dataframe\n",
        "columns = [\"Emp_name\", \"Depart\", \"Age\", \"Salary\"]\n",
        "\n",
        "# creating a dataframe df\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# show df\n",
        "df.show()\n",
        "\n",
        "# counting the total number of values in df\n",
        "print(\"Total number of records in df:\", df.count())"
      ],
      "metadata": {
        "id": "xGoNck--7XWz",
        "outputId": "48af7ca7-19ac-498a-b7b5-94f1df9b7b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+---+------+\n",
            "|Emp_name|    Depart|Age|Salary|\n",
            "+--------+----------+---+------+\n",
            "|     Ram|        IT| 44| 80000|\n",
            "|   Shyam|     Sales| 45| 70000|\n",
            "|    Jiya|     Sales| 30| 60000|\n",
            "|   Maria|  Accounts| 29| 65000|\n",
            "|     Ram|        IT| 38| 80000|\n",
            "|    John|Management| 35| 80000|\n",
            "|   Shyam|     Sales| 45| 70000|\n",
            "|   Kumar|     Sales| 27| 70000|\n",
            "|   Maria|  Accounts| 32| 65000|\n",
            "|     Ria|Management| 32| 65000|\n",
            "+--------+----------+---+------+\n",
            "\n",
            "Total number of records in df: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "\n",
        "# importing sparksession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving an app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# list  of students  data\n",
        "data = [[\"1\", \"sravan\", \"vignan\"], [\"2\", \"ojaswi\", \"vvit\"],\n",
        "        [\"3\", \"rohith\", \"vvit\"], [\"4\", \"sridevi\", \"vignan\"],\n",
        "        [\"1\", \"sravan\", \"vignan\"], [\"5\", \"gnanesh\", \"iit\"]]\n",
        "\n",
        "# specify column names\n",
        "columns = ['student ID', 'student NAME', 'college']\n",
        "\n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Actual data in dataframe\")\n",
        "# show dataframe\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlF945GjiN3l",
        "outputId": "5655004b-c982-4ccb-f890-181559da3ad1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data in dataframe\n",
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import element_at, split, col\n",
        "\n",
        "df = spark.createDataFrame([('a b c d',)], ['s',])\n",
        "\n",
        "df.withColumn('arr', split(df.s, ' ')) \\\n",
        "  .select( col('arr')[0].alias('0th')\n",
        "         , col('arr')[3].alias('3rd')\n",
        "         , element_at(col('arr'), -1).alias('1st_from_end')\n",
        "     ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Y3R1NuicpE",
        "outputId": "9111e34f-edce-49ca-a700-38700aa62038"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+------------+\n",
            "|0th|3rd|1st_from_end|\n",
            "+---+---+------------+\n",
            "|  a|  d|           d|\n",
            "+---+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "df = spark.createDataFrame([('a b c d',)], ['s',])\n",
        "df.select(   split(df.s, ' ')[0].alias('0th'),\n",
        "             split(df.s, ' ')[3].alias('3rd'),\n",
        "             split(df.s, ' ')[-1].alias('1st_from_end')\n",
        "         ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDAC-am9igwk",
        "outputId": "554687f8-d4a6-4adb-b17a-9b71de478dfe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+------------+\n",
            "|0th|3rd|1st_from_end|\n",
            "+---+---+------------+\n",
            "|  a|  d|        NULL|\n",
            "+---+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "from pyspark.sql.functions import split, size\n",
        "from pyspark.sql import Column\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "\n",
        "data = [\n",
        "    ('filename', 's3:/hello/no.csv'),\n",
        "    ('filename', 's3:/hello/why.csv')\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('path', StringType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "\n",
        "\n",
        "def expression_last_item_of_array(split_column: str, split_delimeter: str) -> Column:\n",
        "    \"\"\"\n",
        "        Given column name and delimeter, return expression\n",
        "        for splitting string and returning last item of the array.\n",
        "\n",
        "        Args:\n",
        "            split_column: str\n",
        "            split_delimeter: str\n",
        "\n",
        "        Returns:\n",
        "            pysaprk.sql.Column\n",
        "    \"\"\"\n",
        "    expression = split(split_column, split_delimeter)\n",
        "    n = size(expression)\n",
        "    last = expression.getItem(n - 1)\n",
        "    return last, n\n",
        "\n",
        "last, n = expression_last_item_of_array('path', '/')\n",
        "df.show(),\n",
        "df.select(last.alias('last_element'), n.alias('n_items')).show(), df.select(last.alias('last_element')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKjiUjMPilJr",
        "outputId": "c4b7beb5-c7e0-4e09-9f9c-75546b6eb0d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------+\n",
            "|    name|             path|\n",
            "+--------+-----------------+\n",
            "|filename| s3:/hello/no.csv|\n",
            "|filename|s3:/hello/why.csv|\n",
            "+--------+-----------------+\n",
            "\n",
            "+------------+-------+\n",
            "|last_element|n_items|\n",
            "+------------+-------+\n",
            "|      no.csv|      3|\n",
            "|     why.csv|      3|\n",
            "+------------+-------+\n",
            "\n",
            "+------------+\n",
            "|last_element|\n",
            "+------------+\n",
            "|      no.csv|\n",
            "|     why.csv|\n",
            "+------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a temporary view of\n",
        "# Dataframe and storing it into df2\n",
        "df.createOrReplaceTempView(\"df2\")\n",
        "\n",
        "# using the SQL query to count all\n",
        "# distinct records and display the\n",
        "# count on the screen\n",
        "spark.sql(\"select count(distinct(*)) from df2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb_482-AitFU",
        "outputId": "69f805a7-1a14-47e4-abb0-f1327b7bd79c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|count(DISTINCT name, path)|\n",
            "+--------------------------+\n",
            "|                         2|\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the SQL query to count distinct\n",
        "# records in 2 columns only display the\n",
        "# count on the screen\n",
        "spark.sql(\"select count(distinct(name, path)) from df2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7w-Wul4ix9L",
        "outputId": "478a280b-990e-4d9d-a1cb-5ef41bda7517"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------+\n",
            "|count(DISTINCT named_struct(name, name, path, path))|\n",
            "+----------------------------------------------------+\n",
            "|                                                   2|\n",
            "+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select single and multiple columns\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql import SparkSession\n",
        "#import findspark\n",
        "\n",
        "\n",
        "#findspark.init('c:/spark')\n",
        "\n",
        "# Initialize our data\n",
        "data2 = [(\"Pulkit\", 12, \"CS32\", 82, \"Programming\"),\n",
        "         (\"Ritika\", 20, \"CS32\", 94, \"Writing\"),\n",
        "         (\"Atirikt\", 4, \"BB21\", 78, None),\n",
        "         (\"Reshav\", 18, None, 56, None)\n",
        "         ]\n",
        "\n",
        "# Start spark session\n",
        "spark = SparkSession.builder.appName(\"Student_Info\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Roll Number\", IntegerType(), True),\n",
        "    StructField(\"Class ID\", StringType(), True),\n",
        "    StructField(\"Marks\", IntegerType(), True),\n",
        "    StructField(\"Extracurricular\", StringType(), True)\n",
        "])\n",
        "\n",
        "# read the dataframe\n",
        "df = spark.createDataFrame(data=data2, schema=schema)\n",
        "\n",
        "# slelct columns\n",
        "df.select(\"Name\", \"Marks\").show()\n",
        "\n",
        "# stop the session\n",
        "# spark.stop() # Removed spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWfphY9_jC7E",
        "outputId": "9d6d4f40-02d4-46fe-bf99-73b62c96e806"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Marks|\n",
            "+-------+-----+\n",
            "| Pulkit|   82|\n",
            "| Ritika|   94|\n",
            "|Atirikt|   78|\n",
            "| Reshav|   56|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "605b81e1"
      },
      "source": [
        "%pip install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(col(\"Name\"),col(\"Marks\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACrE3ktujsdc",
        "outputId": "150e7d70-41d5-40b6-ce0f-6173203ba896"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Marks|\n",
            "+-------+-----+\n",
            "| Pulkit|   82|\n",
            "| Ritika|   94|\n",
            "|Atirikt|   78|\n",
            "| Reshav|   56|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select spark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql import SparkSession\n",
        "#import findspark\n",
        "\n",
        "\n",
        "#findspark.init('c:/spark')\n",
        "\n",
        "# Initialize our data\n",
        "data2 = [(\"Pulkit\", 12, \"CS32\", 82, \"Programming\"),\n",
        "         (\"Ritika\", 20, \"CS32\", 94, \"Writing\"),\n",
        "         (\"Atirikt\", 4, \"BB21\", 78, None),\n",
        "         (\"Reshav\", 18, None, 56, None)\n",
        "         ]\n",
        "\n",
        "# Start spark session\n",
        "spark = SparkSession.builder.appName(\"Student_Info\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Roll Number\", IntegerType(), True),\n",
        "    StructField(\"Class ID\", StringType(), True),\n",
        "    StructField(\"Marks\", IntegerType(), True),\n",
        "    StructField(\"Extracurricular\", StringType(), True)\n",
        "])\n",
        "\n",
        "# read the dataframe\n",
        "df = spark.createDataFrame(data=data2, schema=schema)\n",
        "\n",
        "# select the columns\n",
        "df.select(df.columns[:4]).show()\n",
        "\n",
        "# stop session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoCEwtkxj88b",
        "outputId": "8116f4c1-069d-43ce-9b7e-a98e67ff92b1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------+-----+\n",
            "|   Name|Roll Number|Class ID|Marks|\n",
            "+-------+-----------+--------+-----+\n",
            "| Pulkit|         12|    CS32|   82|\n",
            "| Ritika|         20|    CS32|   94|\n",
            "|Atirikt|          4|    BB21|   78|\n",
            "| Reshav|         18|    NULL|   56|\n",
            "+-------+-----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql import SparkSession\n",
        "#import findspark\n",
        "\n",
        "\n",
        "#findspark.init('c:/spark')\n",
        "\n",
        "# initialize the data\n",
        "data = [\n",
        "    ((\"Pulkit\", \"Dhingra\"), 12, \"CS32\", 82, \"Programming\"),\n",
        "    ((\"Ritika\", \"Pandey\"), 20, \"CS32\", 94, \"Writing\"),\n",
        "    ((\"Atirikt\", \"Sans\"), 4, \"BB21\", 78, None),\n",
        "    ((\"Reshav\", None), 18, None, 56, None)\n",
        "]\n",
        "\n",
        "# start spark session\n",
        "spark = SparkSession.builder.appName(\"Student_Info\").getOrCreate()\n",
        "\n",
        "# initialize the schema of the data\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "        StructField('firstname', StringType(), True),\n",
        "        StructField('lastname', StringType(), True)\n",
        "    ])),\n",
        "    StructField(\"Roll Number\", IntegerType(), True),\n",
        "    StructField(\"Class ID\", StringType(), True),\n",
        "    StructField(\"Marks\", IntegerType(), True),\n",
        "    StructField(\"Extracurricular\", StringType(), True)\n",
        "])\n",
        "# create a dataframe\n",
        "df2 = spark.createDataFrame(data=data, schema=schema)\n",
        "\n",
        "# display the schema\n",
        "df2.printSchema()\n",
        "\n",
        "# select operation\n",
        "df2.select(\"name.firstname\", \"name.lastname\").show(truncate=False)\n",
        "\n",
        "# stop session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MREvGUzjl1-L",
        "outputId": "03dcf572-7fbe-4102-bbe1-380964b6b10f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- Roll Number: integer (nullable = true)\n",
            " |-- Class ID: string (nullable = true)\n",
            " |-- Marks: integer (nullable = true)\n",
            " |-- Extracurricular: string (nullable = true)\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|Pulkit   |Dhingra |\n",
            "|Ritika   |Pandey  |\n",
            "|Atirikt  |Sans    |\n",
            "|Reshav   |NULL    |\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing module\n",
        "import pyspark\n",
        "\n",
        "# importing sparksession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# creating sparksession and giving an app name\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "# list  of students  data\n",
        "data = [[\"1\", \"sravan\", \"vignan\"], [\"2\", \"ojaswi\", \"vvit\"],\n",
        "        [\"3\", \"rohith\", \"vvit\"], [\"4\", \"sridevi\", \"vignan\"],\n",
        "        [\"1\", \"sravan\", \"vignan\"], [\"5\", \"gnanesh\", \"iit\"]]\n",
        "\n",
        "# specify column names\n",
        "columns = ['student ID', 'student NAME', 'college']\n",
        "\n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Actual data in dataframe\")\n",
        "# show dataframe\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j_EZDHgl9nb",
        "outputId": "ce33d0e9-b144-4001-c3af-cf70b52b7f2f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data in dataframe\n",
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column name\n",
        "dataframe.select('student ID').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhSY9KdymAqa",
        "outputId": "3e64a63e-584f-4d5e-ffaa-a29585303b73"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|student ID|\n",
            "+----------+\n",
            "|         1|\n",
            "|         2|\n",
            "|         3|\n",
            "|         4|\n",
            "|         1|\n",
            "|         5|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select multiple column with column name\n",
        "dataframe.select(['student ID', 'student NAME', 'college']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R316VMDAmEsT",
        "outputId": "f482cab6-318f-4d80-f50e-efb6a767bb83"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column number 1\n",
        "dataframe.select(dataframe.columns[1]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJvx1vyymMfS",
        "outputId": "b3ec7c73-b95d-43c8-aa70-0211d1a2629e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|student NAME|\n",
            "+------------+\n",
            "|      sravan|\n",
            "|      ojaswi|\n",
            "|      rohith|\n",
            "|     sridevi|\n",
            "|      sravan|\n",
            "|     gnanesh|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select column with column number slice\n",
        "# operator\n",
        "dataframe.select(dataframe.columns[0:3]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtfqadvgmQZy",
        "outputId": "faf5a980-05c2-442f-e292-59554189d894"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student ID|student NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|      sravan| vignan|\n",
            "|         2|      ojaswi|   vvit|\n",
            "|         3|      rohith|   vvit|\n",
            "|         4|     sridevi| vignan|\n",
            "|         1|      sravan| vignan|\n",
            "|         5|     gnanesh|    iit|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import current_date,year\n",
        "\n",
        "from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n",
        "\n",
        "from datetime import datetime, date\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc=SparkContext.getOrCreate()\n",
        "\n",
        "rdd=sc.parallelize([('ram','chi'),\n",
        "\n",
        "             ('anil','ind')])\n",
        "\n",
        "df=spark.createDataFrame(rdd,schema=StructType([StructField(\"name\",StringType(),True),StructField(\"loc\",StringType(),True)]))\n",
        "\n",
        "\n",
        "\n",
        "#Trying to see the data but face below error:\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-cJJqPpHDT",
        "outputId": "bb462e2d-fe57-4816-ef60-1e2f15547bfd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|name|loc|\n",
            "+----+---+\n",
            "| ram|chi|\n",
            "|anil|ind|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session with additional configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Demo Fix\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Perform a transformation and show results\n",
        "try:\n",
        "    print(\"Attempting to display DataFrame...\")\n",
        "    df.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error in df.show(): {str(e)}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxlOxHdI8ESh",
        "outputId": "c9f5ca7e-8c75-4bdb-9c30-2938ec1d0dbe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to display DataFrame...\n",
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    ('2024-01-01', 'ProductA', 10),\n",
        "    ('2024-01-01', 'ProductB', 20),\n",
        "    ('2024-01-02', 'ProductA', 30),\n",
        "    ('2024-01-02', 'ProductB', 40),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, ['date', 'product', 'sales'])\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Pivot DataFrame\n",
        "pivot_df = df.groupBy('date').pivot('product').agg(sum('sales'))\n",
        "\n",
        "print(\"Dataframe after Pivot Operation\")\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5uX3XCQ8-hl",
        "outputId": "06c02351-8ebc-44c0-d56b-1415076d97f4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----+\n",
            "|      date| product|sales|\n",
            "+----------+--------+-----+\n",
            "|2024-01-01|ProductA|   10|\n",
            "|2024-01-01|ProductB|   20|\n",
            "|2024-01-02|ProductA|   30|\n",
            "|2024-01-02|ProductB|   40|\n",
            "+----------+--------+-----+\n",
            "\n",
            "Dataframe after Pivot Operation\n",
            "+----------+--------+--------+\n",
            "|      date|ProductA|ProductB|\n",
            "+----------+--------+--------+\n",
            "|2024-01-02|      30|      40|\n",
            "|2024-01-01|      10|      20|\n",
            "+----------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# function to create new SparkSession\n",
        "def create_session():\n",
        "    spk = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Product_details.com\") \\\n",
        "        .getOrCreate()\n",
        "    return spk\n",
        "\n",
        "def create_df(spark, data, schema):\n",
        "    df1 = spark.createDataFrame(data, schema)\n",
        "    return df1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_data = [(\"Uttar Pradesh\", 122000, 89600, 12238),\n",
        "                  (\"Maharashtra\", 454000, 380000, 67985),\n",
        "                  (\"Tamil Nadu\", 115000, 102000, 13933),\n",
        "                  (\"Karnataka\", 147000, 111000, 15306),\n",
        "                  (\"Kerala\", 153000, 124000, 5259)]\n",
        "\n",
        "    # calling function to create SparkSession\n",
        "    spark = create_session()\n",
        "\n",
        "    schema = [\"State\", \"Cases\", \"Recovered\", \"Deaths\"]\n",
        "\n",
        "    # calling function to create dataframe\n",
        "    df = create_df(spark, input_data, schema)\n",
        "\n",
        "    # visualizing the dataframe\n",
        "    df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQYRCdqY9FQs",
        "outputId": "96715bd8-5991-48b2-e661-a62d5756c05b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the list of column names\n",
        "col = df.columns\n",
        "\n",
        "# printing\n",
        "print(f'List of column names: {col}')\n",
        "\n",
        "# visualizing the dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjM6EqZ-9QEd",
        "outputId": "fc889399-f4a8-4679-9707-36d40ce91034"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of column names: ['State', 'Cases', 'Recovered', 'Deaths']\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the list of StructFields\n",
        "field = df.schema.fields\n",
        "\n",
        "# using for loop to iterate and enumerate\n",
        "# for indexing or numbering\n",
        "for count, col_name in enumerate(field, 1):\n",
        "\n",
        "    # printing the column names\n",
        "    print(count, \"-\", col_name.name)\n",
        "\n",
        "    # visualizing the dataframe\n",
        "    df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26F4AMBe9Ue1",
        "outputId": "b84279f1-23b2-44e8-dbc7-1c35212e6102"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 - State\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n",
            "2 - Cases\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n",
            "3 - Recovered\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n",
            "4 - Deaths\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing Dataframe schema to\n",
        "# get the column names\n",
        "df.printSchema()\n",
        "\n",
        "# visualizing the dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4bn0k0b9aEG",
        "outputId": "b26837dd-5f5d-4dfd-80bc-9c14a65c7f8e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- State: string (nullable = true)\n",
            " |-- Cases: long (nullable = true)\n",
            " |-- Recovered: long (nullable = true)\n",
            " |-- Deaths: long (nullable = true)\n",
            "\n",
            "+-------------+------+---------+------+\n",
            "|        State| Cases|Recovered|Deaths|\n",
            "+-------------+------+---------+------+\n",
            "|Uttar Pradesh|122000|    89600| 12238|\n",
            "|  Maharashtra|454000|   380000| 67985|\n",
            "|   Tamil Nadu|115000|   102000| 13933|\n",
            "|    Karnataka|147000|   111000| 15306|\n",
            "|       Kerala|153000|   124000|  5259|\n",
            "+-------------+------+---------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}